# The Secret Life of Software Vulnerabilities: A Large-Scale Empirical Study

This `README.md` file contains (1) a description of the datasets employed and (2) the steps needed to re-run the data collection, extraction, and analyses.

This package is made of three main directories:

- `data/` containing all the datasets employed;
- `scripts/` containing all the Python scripts, organized in modules;
- `results/` containing all the results of the data analyses.

## Data

Here we reported a brief description of each dataset in `data/`:

### Source Datasets

- `nvd_raw.json` contains the raw export of NVD (from [here](https://www.cve-search.org/dataset/)) in date **1st November 2020**. This dataset was used as a basis for the entire study.
- `cves.json` contains the pretty printed CVE data extracted from `nvd_raw.json` without the unneeded attributes. Schema:
  - `cve` - CVE ID
  - `cwe` - Related CWE
  - `cvss` - CVSS Score
  - `cvss-vector` - CVSS Vector
  - `fixes` - List of fixing commits
    - `repo` - GitHub repository URL
    - `hash` - Fixing commit hash
- `repos_enriched.json` contains details of all the repositories considered in this study. Schema:
  - `repo` - GitHub repository URL
  - `creation_date` - Date of the first commit (`YYYY-MM-DD HH:MM:SS±HH:MM`)
  - `commits` - Number of commits
  - `goals` - The number of commits divided by the four commit categories (used for RQ2)
  - `authors` - Number of different contributors
  - `stars` - Number of GitHub stars
  - `forks` - Number of GitHub forks
  - `language` - The main programming language detected by GitHub
  - `cves` - List of CVEs affecting the project
    - `cve` - CVE ID
    - `cwe` - Related CWE
    - `cvss` - CVSS Score
    - `cvss-vector` - CVSS Vector
    - `fixes` - List of fixing commits
      - `hash` - Fixing commit hash
      - `message` - Fixing commit message
      - `date` - Fixing commit author date (YYYY-MM-DD HH:MM:SS±HH:MM)
      - `author` - Fixing commit author email
      - `files` - List of modified filenames, containing only the **valid** ones
      - `added_lines` - Total number of added lines to `files`
      - `removed_lines` - Total number of removed lines from `files`
      - `vccs` - List of VCCs
        - `hash` - VCC hash
        - `message` - VCC message
        - `date` - VCC author date (YYYY-MM-DD HH:MM:SS±HH:MM)
        - `author` - VCC author email
        - `author_commits_workload` - VCC author commit workload
        - `author_churn_workload` - VCC author churn workload
        - `author_tenure` - VCC author tenure
        - `nearest_release_name` - Name of the tag of the first release containing the VCC
        - `nearest_release_date` - Date of the first release containing the VCC (YYYY-MM-DD HH:MM:SS±HH:MM)
        - `added_lines` - Total number of added lines to **all** changed files
        - `removed_lines` - Total number of removed lines to **all** changed files
        - `commits_before_fix` - Number of commits between this VCC and the related fixing commit
        - `files` - List of files, involved in the related fixing commit, that were modified in the VCC:
          - `name` - Filename
          - `changes_before_fix` - Number of commits that touched the file from the VCC to the related fixing commit
          - `previous_changes` - Number of commits that touched the file before the VCC
        - `changes_before_fix` - Number of commits that touched any of the file in `files` from the VCC to the related fixing commit
      - `commits_before_last_fix` - Number of commits between this fixing commit and the last fixing commit
      - `changes_before_last_fix` - Number of commits that touched any of the files in `files` between this fixing commit and the last fixing commit

### Validation

- `cve_vcc_pairs_population.csv` contains the entire population pairs (CVE, VCC) coming from the VCCs extraction algorithm
- `cve_vcc_pairs_sample.csv` contains the statistically significant sample of 376 pairs (CVE, VCC) coming from the VCCs extraction algorithm
- `alignment.csv` contains the outcome of the initial alignment round
- `vccs_mining_validation.csv` contains the outcome of the manual validation of the VCCs extraction algorithm
- `fix_cve_pairs_population.csv` contains the entire population pairs (Fix, CVE) used in RQ4
- `fix_cve_pairs_sample.csv` contains the statistically significant sample of 351 pairs (Fix, CVE) used in RQ4

### Input for Analysis Scripts

- `top25cwe.csv` contains the ordered Top Most Dangerous 25 CWE 2020, extracted from CWE website.
- `removal_methods.csv` contains the outcome of the manual classification of vulnerability removal methods done in RQ4.
- `rq1_input.csv` contains the data specifically needed for `rq1.py`, i.e., details of all VCCs w.r.t. files
- `rq2_input.csv` contains the data specifically needed for `rq2.py`, i.e., details of all VCCs w.r.t. process aspects
  - `rq2_discarded.csv` contains the VCCs unsuitable for `rq2.py`
- `rq3_input.csv` contains the data specifically needed for `rq3.py`, i.e., details of all CVEs
  - `rq3_discarded.csv` contains the CVEs unsuitable for `rq3.py`
- `rq4_input.csv` contains the data specifically needed for `rq4.py`, i.e., details of all fixing commits

## Reproduction Steps

Here we explain each step that involves running the scripts.

Basic requirements:

- `git`
- `python`
- `pip`

All the main scripts are in `scripts/` directory. Before starting, it is suggested to install the required Python packages: `pip install -r requirements.txt`.

The scripts are divided into five modules:

- **Data Collection** (`data_collection/`), for parsing the input dataset (`nvd_raw.json`) and clone all the repositories in a specific directory. 
- **Data Extraction** (`data_extraction/`), for mining the VCCs and enrich all the commits with the necessary data.
- **Data Analysis** (`data_analysis/`), for printing statistics and plotting graphs about the mined data.
- **Common** (`common/`), with useful helper functions.
- **Validation** (`validation/`), for analyzing the results of the VCCs mining procedure validation.
- **Sanity** (`sanity/`), containing the script for making the sanity check.

### Data Collection

Scripts:

- `extract_cve.py` reads the content in `nvd_raw.json` and reorganizes it as `cves.json`. The execution may take **several minutes**. Note that `cwe` data are mined with separate API calls, so this script requires an Internet connection. 
- `clone_repos.py <dest> <reposEnriched> <reposDiscarded>` clones the repositories found in `cves.json`. The script accepts three arguments:
  - `<dest>` the directory that will contain the cloned repository.
  - `<reposEnriched>` the path of a `repos_enriched.json` containing the previously-enriched repositories, to avoid cloning them again;
  - `<reposDiscarded>` the path of a `repos_discarded.json` containing the discarded set of repositories, to avoid cloning them again.

 The script requires an Internet connection to fetch the repositories from GitHub. All the repositories should take about 200 GB. **Make sure there is enough space on your storage to host all the repositories**.

### Data Extraction

#### VCCs Mining and Enrichment

The main script for the data extraction is `enrich_repos.py <reposLocation> <reposEnriched> <reposDiscarded> <GitHubUsername:GitHubToken>`, which accepts four arguments:
- `<reposLocation>` the directory containing the cloned repositories the script is going to analyze;
- `<reposEnriched>` the path of a `repos_enriched.json` file we are willing to fill with enriched repositories;
- `<reposDiscarded>` the path of a `repos_discarded.json` file we are willing to fill with discarded repositories;
- `<GitHubUsername:GitHubToken>` a string containing the GitHub username and API token needed to make API calls (as a consequence, the script requires an Internet connection).

This script works incrementally, so it can be interrupted with `CTRL+C` and resumed at a different time, as it avoids the re-extraction of already-enriched or discarded repositories.

The execution may take **many and many hours** for certain metrics, mainly due to repositories with a large history. Please note that the `/tmp` size should be increased to (at least) 16GB: `mount -o remount,size=16G /tmp`, in order to host large repositories (e.g., `liferay_liferay/portal`). 

The extraction process ignores the following files:

- All files belonging to the following extensions: `{'.txt', '.md', '.man', '.lang', '.loc', '.tex', '.texi', '.rst', '.gif', '.png', '.jpg', '.jpeg', '.svg', '.ico', '.css', '.scss', '.less', '.gradle', '.ini', '.zip', '.pdf'}`
- All files matching the following (Python) regex: `r"^(install|changelog(s)?|change(s)?|author(s)?|news|readme|todo|about(s)?|credit(s)?|license|release(s)?|release(s)?|release(_|-)note(s)?|version(s)?|makefile|pom|\.git.*|\.travis|\.classpath|\.project)$"`
- All test files

### Data Analysis

#### Data Preparation

The script `data_preparation.py` generates the input files needed by the four RQ scripts. Namely:

- `projects_info.csv` containing general information of all the considered projects;
- `rq1_input.csv`, containing the input to answer RQ1;
- `rq2_input.csv`, containing the input to answer RQ2;
- `rq3_input.csv`, containing the input to RQ3;
- `rq4_input.csv`, containing the input to RQ4.

It accepts two command-line arguments:

- `<input_dir>` the directory containing the input files for the RQs
- `<output_dir>` the directory that will contain the summaries of project-level metrics.

For the sake of completeness, it also creates the files `rq2_discarded.csv` and `rq3_discarded.csv`, containing the discarded data not needed by these two RQs.

#### Analysis Scripts

The following scripts reproduce the analyses made in each of the four RQs:

- `rq1.py`
- `rq2.py`
- `rq3.py`
- `rq4.py`

All of them accept two command-line arguments:

- `<input_dir>` the directory containing the input files for each RQ
- `<output_dir>` the directory that will contain the results of each RQ.

### VCCs Mining Validation

The script `sampling.py` reproduces the selection of the statistically significant sample of (CVE, VCC), used for the manual validation of our VCCs mining algorithm. The population and the sample are put into `data/validation/` directory, where also the outcome of the manual validation can be found `vccs_mining_validation.csv`.

The script `agreement.r` computes the agreement measures on the `alignment.csv` file, containing the outcome of the initial alignment round. The output is placed into `results/validation`.

## Categorization Keywords (RQ2)

- New Feature: `{new, feature, add, creat, introduc, implement}`
- Bug Fix: `{fix, repair, error, bug, issue, exception}`
- Enhancement: `{updat, modif, upgrad, export, remov, integrat, support, enhancement, replac, includ, expos, generat, migrat}`
- Refactoring: `{renam, reorganiz, refactor, clean, polish, mov, extract, reorder, re-order, merg}`

## Removal Methods Taxonomy (RQ4)

### Users and Networking

- **Hide Sensitive Information**: Do not allow passwords/tokens/keys to be given to users (e.g., via outputs, stack traces, or source code).
- **Change User Permissions**: Change the assignment of either intended or unintended functionalities or ports to the different users/actors.
- **Block Untrusted Hosts**: Remove or block hosts considered untrusted.
- **Limit Attempts**: Limit the attempts to sensitive functionalities, such as logins or payments.
- **Improve Certificate Verification**: Check the certificate validity (its digital signature and origin).
- **Improve Session Management**: Set a session timeout, invalidate the session on logout, or reuse a pre-existing session only when needed.
- **Ask User Confirmation**: Add a confirm dialog or an extra input to avoid issuing operations unintentionally.

### Memory Management

- **Prevent Access Over Bounds**: Ensure a pointer's offset does not go beyond the buffer limit or check if the signed integer buffer's length does not overflow, e.g., when used in fread().
- **Check Before Dereferencing**: Check if a pointer is not NULL before dereferencing it, or if a file is not a symlink.
- **Check Input Size**: Check the size (bytes) of externally supplied data before placing it in a buffer.
- **Change Buffer Size**: Increase/decrease the size of a dynamically allocated buffer (e.g., malloc()-like functions) or statically allocated variable.
- **Improve Resource Management**: Set a limit to the number of allocated resources (e.g., TCP sack holes, file descriptors, etc.); avoid opening them when not strictly needed or close them when not needed anymore; cleanup memory leaks.
- **Remove Invalid Free**: Remove free() calls on non-dynamically allocated buffers or avoid doing double free.
- **Set Pointer To Null After Free**: Set a pointer to NULL after its use in a free().
- **Remove Type Confusion**: Remove cases where a resource (e.g., a buffer) is accessed improperly (e.g., with unions or wrong pointer types).

### Implementation

- **Sanitize External Input**: Add or improve checks on the content of externally supplied data, either rejecting it or adjusting the content (e.g., escaping/encoding special characters, remove decimals, relying on parameterization).
- **Handle Error Cases**: Handle exceptions/signals/error return values previously not considered. This comprises loop exit conditions, division-by-zero, and integer under/overflows, as well.
- **Fix Initialization**: Put objects/structs/buffers in a proper initial state, e.g., when creating an object starting from another (clone).
- **Employ New Algorithm**: Employ a brand new algorithm/mechanism for checking security-related aspects (e.g., CSRF protection), either from scratch or third-party solutions.
- **Remove Vulnerable Code**: Remove the vulnerable feature/file code completely.
- **Remove Race Conditions**: Fix concurrency issues, e.g., improving the locks management.
- **Avoid Deserialization of Untrusted And Harmful Data**: Deserialize only the unharmful part of untrusted data or prevent it at all.

### Configuration

- **Improve Security Configuration**: Modify security-related configuration (e.g., framework settings, configuration files, dependencies).
