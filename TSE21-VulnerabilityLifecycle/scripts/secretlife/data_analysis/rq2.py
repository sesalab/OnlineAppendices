import os
import sys

import pandas as pd
from scipy.stats import spearmanr


if len(sys.argv) < 2:
    print("Need to specify the directory containing the input data. Exiting.")
    sys.exit(1)
input_dir = sys.argv[1] # e.g., ../../../data/
if len(sys.argv) < 3:
    print("Need to specify the directory containing the output data. Exiting.")
    sys.exit(1)
output_dir = sys.argv[2] # e.g., ../../../results/

input_filepath = os.path.join(input_dir, "rq2_input.csv")
projects_info_filepath = os.path.join(input_dir, "projects_info.csv")
top_25_filepath = os.path.join(input_dir, "top25cwe.csv")
output_filepath = os.path.join(output_dir, "rq2/output.txt")

distinct_goals_per_cve_overall_filepath = os.path.join(output_dir, "rq2/distinct_goals_per_cve_overall.csv")
refactorings_ratio_overall_filepath = os.path.join(output_dir, "rq2/refactorings_ratio_overall.csv")

goal_columns = ["new_feature", "bug_fixing", "enhancement", "refactoring"]
distance_columns = ["distance_from_release", "distance_from_creation"]

df_vccs = pd.read_csv(input_filepath, delimiter=",")
top_25 = pd.read_csv(top_25_filepath)[["cwe", "position"]]
df_vccs_top_25 = df_vccs[df_vccs["cwe"].isin(top_25["cwe"])]
df_vccs_top_25_groups = df_vccs_top_25.groupby("cwe", sort=False)
cwe_order = top_25.sort_values("position", ignore_index=True)["cwe"].tolist()
projects_info = pd.read_csv(projects_info_filepath)

cve_groups = df_vccs.groupby("cve", sort=False)
cves_number_vccs = cve_groups.size()
cves_number_vccs.name = "vccs_count"
cves_goals = cve_groups[goal_columns].sum()
cves_min_distances = cve_groups[distance_columns].min()
cves_avg_commit_workload = cve_groups["author_commits_workload"].mean()
cves_avg_churn_workload = cve_groups["author_churn_workload"].mean()
cves_avg_tenure = cve_groups["author_tenure"].mean()
severity = cve_groups["cvss"].first()
severity.name = "severity"
df_cves = pd.concat([cves_number_vccs, cves_goals, cves_min_distances, cves_avg_commit_workload, cves_avg_churn_workload, cves_avg_tenure, severity], axis=1)


def commit_goals(df_vccs):
    no_goals_count = (df_vccs[goal_columns].sum(axis=1) == 0).sum()
    new_feature_count = df_vccs[goal_columns[0]].sum()
    bug_fixing_count = df_vccs[goal_columns[1]].sum()
    enhancement_count = df_vccs[goal_columns[2]].sum()
    refactoring_count = df_vccs[goal_columns[3]].sum()
    total_vccs = len(df_vccs)
    no_goals_perc = no_goals_count / total_vccs * 100
    new_feature_perc = new_feature_count / total_vccs * 100
    bug_fixing_perc = bug_fixing_count / total_vccs * 100
    enhancement_perc = enhancement_count / total_vccs * 100
    refactoring_perc = refactoring_count / total_vccs * 100
    refactorings = pd.merge(df_vccs, projects_info, left_on="repo", right_on="url")[["repo", "refactoring_y"]].drop_duplicates(ignore_index=True)["refactoring_y"]
    total_refactorings = refactorings.sum()
    if total_refactorings > 0:
        refactoring_over_total_perc = refactoring_count / total_refactorings * 100
    else:
        refactoring_over_total_perc = 0
    out_string = ""
    out_string += f"** VCCs w/o any goal: {no_goals_count}/{total_vccs} ({round(no_goals_perc, 2)}%)\n"
    out_string += "** VCCs w/ goals:\n"
    out_string += f"   - New Feature: {bug_fixing_count}/{total_vccs} ({round(bug_fixing_perc, 2)}%)\n"
    out_string += f"   - Bug Fix: {new_feature_count}/{total_vccs} ({round(new_feature_perc, 2)}%)\n"
    out_string += f"   - Enhancement: {enhancement_count}/{total_vccs} ({round(enhancement_perc, 2)}%)\n"
    out_string += f"   - Refactoring: {refactoring_count}/{total_vccs} ({round(refactoring_perc, 2)}%)\n"
    out_string += f"     - On Total Refactoring Commits: {refactoring_count}/{total_refactorings} ({round(refactoring_over_total_perc, 2)}%)\n"
    return out_string


def refactoring_ratio(df_vccs):
    refactoring_count = df_vccs[goal_columns[3]].sum()
    refactorings = pd.merge(df_vccs, projects_info, left_on="repo", right_on="url")[["repo", "refactoring_y"]].drop_duplicates(ignore_index=True)["refactoring_y"]
    total_refactorings = refactorings.sum()
    if total_refactorings > 0:
        refactorings_ratio = refactoring_count / total_refactorings
    else:
        refactorings_ratio = 0
    return refactorings_ratio


def project_startup(df_vccs):
    same_week_count = (df_vccs[distance_columns[1]] <= 7).sum()
    week_after_count = ((df_vccs[distance_columns[1]] > 7) & (df_vccs[distance_columns[1]] <= 28)).sum()
    month_after_count = ((df_vccs[distance_columns[1]] > 28) & (df_vccs[distance_columns[1]] <= 365)).sum()
    over_year_count = (df_vccs[distance_columns[1]] > 365).sum()
    total_vccs = len(df_vccs)
    same_week_perc = same_week_count / total_vccs * 100
    week_after_perc = week_after_count / total_vccs * 100
    month_after_perc = month_after_count / total_vccs * 100
    over_year_perc =  over_year_count / total_vccs * 100
    out_string = ""
    out_string += f"  - the same week of: {same_week_count}/{total_vccs} ({round(same_week_perc, 2)}%)\n"
    out_string += f"  - the week after: {week_after_count}/{total_vccs} ({round(week_after_perc, 2)}%)\n"
    out_string += f"  - the month after: {month_after_count}/{total_vccs} ({round(month_after_perc, 2)}%)\n"
    out_string += f"  - over one year after: {over_year_count}/{total_vccs} ({round(over_year_perc, 2)}%)\n"
    out_string += "  the project creation.\n"
    return out_string


def working_on_release(df_vccs):
    same_day_count = (df_vccs[distance_columns[0]] == 0).sum()
    day_before_count = (df_vccs[distance_columns[0]] == 1).sum()
    within_week_count = ((df_vccs[distance_columns[0]] > 1) & (df_vccs[distance_columns[0]] <= 7)).sum()
    within_month_count = ((df_vccs[distance_columns[0]] > 7) & (df_vccs[distance_columns[0]] <= 28)).sum()
    over_month_count = (df_vccs[distance_columns[0]] > 28).sum()
    total_vccs = len(df_vccs)
    same_day_perc = same_day_count / total_vccs * 100
    day_before_perc = day_before_count / total_vccs * 100
    within_week_perc = within_week_count / total_vccs * 100
    within_month_perc = within_month_count / total_vccs * 100
    over_month_perc = over_month_count / total_vccs * 100
    out_string = ""
    out_string += f"  - the same day: {same_day_count}/{total_vccs} ({round(same_day_perc, 2)}%)\n"
    out_string += f"  - the day before: {day_before_count}/{total_vccs} ({round(day_before_perc, 2)}%)\n"
    out_string += f"  - within the week: {within_week_count}/{total_vccs} ({round(within_week_perc, 2)}%)\n"
    out_string += f"  - within the month: {within_month_count}/{total_vccs} ({round(within_month_perc, 2)}%)\n"
    out_string += f"  - over one month before: {over_month_count}/{total_vccs} ({round(over_month_perc, 2)}%)\n"
    out_string += "  of a release.\n"
    return out_string


def split_distribution(df_vccs, column):
    total_vccs = len(df_vccs)
    vccs_low_workload_perc = (df_vccs[column] < 0.25).sum() / total_vccs * 100
    vccs_medium_workload_perc = ((df_vccs[column] >= 0.25) & (df_vccs[column] < 0.75)).sum() / total_vccs * 100
    vccs_high_workload_perc = (df_vccs[column] >= 0.75).sum() / total_vccs * 100
    out_string = ""
    out_string += f"  - Low: {round(vccs_low_workload_perc, 2)}%\n"
    out_string += f"  - Medium: {round(vccs_medium_workload_perc, 2)}%\n"
    out_string += f"  - High: {round(vccs_high_workload_perc, 2)}%\n"
    return out_string


# General Profiling
num_vccs = len(df_vccs)
num_repos = df_vccs["repo"].nunique()
num_cves = len(df_cves)
with open(output_filepath, "w") as output_file:
    print(f"RQ2 Context:", file=output_file)
    print(f"* CVEs: {num_cves}", file=output_file)
    print(f"* VCCs: {num_vccs}", file=output_file)
    print(f"* Repos: {num_repos}", file=output_file)
    print("\n==========\n", file=output_file)
    print("Commit Goals Results:", file=output_file)

# Part 1: Commit Goals Analysis

## CVE Analysis: CVEs goal distribution; CVEs w/o any goal; CVEs w/ all goals
cves_distinct_goals_count = (df_cves[goal_columns] > 0).sum(axis=1).rename("distinct_goals")
cves_distinct_goals_count.describe().to_csv(distinct_goals_per_cve_overall_filepath)
cves_no_goals_count = (df_cves[goal_columns].sum(axis=1) == 0).sum()
cves_all_goals_count = (df_cves[goal_columns].product(axis=1) > 0).sum()
with open(output_filepath, "a") as output_file:
    print(f"* CVEs w/o any commit goal: {cves_no_goals_count}/{num_cves} ({round(cves_no_goals_count / num_cves * 100, 2)}%)", file=output_file)
    print(f"* CVEs w/ all commit goals: {cves_all_goals_count}/{num_cves} ({round(cves_all_goals_count / num_cves * 100, 2)}%)\n", file=output_file)

## (Overall) Frequency Distribution
goals = commit_goals(df_vccs)
with open(output_filepath, "a") as output_file:
    print("* VCC goals (Overall):", file=output_file)
    print(goals, file=output_file)

## (Per CWEs) Frequency Distribution
cwe_goals = {}
for name, group_df in df_vccs_top_25_groups:
    cwe_goals[name] = commit_goals(group_df)
with open(output_filepath, "a") as output_file:
    for cwe in cwe_order:
        if cwe in cwe_goals:
            print(f"* VCC goals ({cwe}):", file=output_file)
            print(cwe_goals[cwe], file=output_file)

## TODO (Opz.) Extend the CVE Analysis to Top 25 CWE?

## Project Analysis: Project's VCC Refactorings Ratio (over Total)
project_goals = []
project_groups = df_vccs.groupby("repo", sort=False)
for name, group_df in project_groups:
    project_goals.append((name, refactoring_ratio(group_df)))
df_ratios = pd.DataFrame.from_records(project_goals, columns=["repo", "refactoring_ratio"])
df_ratios["refactoring_ratio"].describe().to_csv(refactorings_ratio_overall_filepath)

# Part 2: Project Status Analysis

## Part 2.1: Project Startup

### (Overall) Frequency Distribution
startups = project_startup(df_vccs)
with open(output_filepath, "a") as output_file:
    print("==========\n", file=output_file)
    print("Project Startup Results:", file=output_file)
    print("* VCC were made (Overall):", file=output_file)
    print(startups, file=output_file)

## (Per CWEs) Frequency Distribution
cwe_startups = {}
for name, group_df in df_vccs_top_25_groups:
    cwe_startups[name] = project_startup(group_df)
with open(output_filepath, "a") as output_file:
    for cwe in cwe_order:
        if cwe in cwe_startups:
            print(f"* VCC were made ({cwe}):", file=output_file)
            print(cwe_startups[cwe], file=output_file)

## Part 2.2: Working on Release

### (Overall) Frequency Distribution
releases = working_on_release(df_vccs)
with open(output_filepath, "a") as output_file:
    print("==========\n", file=output_file)
    print("Working on Release Results:", file=output_file)
    print("* VCC were made (Overall):", file=output_file)
    print(releases, file=output_file)

## (Per CWEs) Frequency Distribution
cwe_releases = {}
for name, group_df in df_vccs_top_25_groups:
    cwe_releases[name] = working_on_release(group_df)
with open(output_filepath, "a") as output_file:
    for cwe in cwe_order:
        if cwe in cwe_releases:
            print(f"* VCC were made ({cwe}):", file=output_file)
            print(cwe_releases[cwe], file=output_file)

## CVE Analysis: CVEs w/ VCCs very close to release
cves_near_release_count = (df_cves["distance_from_release"] <= 1).sum()
with open(output_filepath, "a") as output_file:
    print(f"CVEs with >= 1 VCC made One Day before or the Same Day of a Release: {cves_near_release_count}/{num_cves} ({round(cves_near_release_count / num_cves * 100, 2)}%)\n", file=output_file)

## TODO (Opz.) Extend the CVE Analysis to Top 25 CWE?

# Part 3: Developers Status Analysis

## Author Commit Workload

### (Overall) Frequency Distribution
workloads = split_distribution(df_vccs, column="author_commits_workload")
with open(output_filepath, "a") as output_file:
    print("==========\n", file=output_file)
    print("Developers Status Results:", file=output_file)
    print("* Author Commit Workloads (Overall):", file=output_file)
    print(workloads, file=output_file)

### (Per CWEs) Frequency Distribution
cwe_commit_workloads = {}
for name, group_df in df_vccs_top_25_groups:
    cwe_commit_workloads[name] = split_distribution(group_df, column="author_commits_workload")
with open(output_filepath, "a") as output_file:
    for cwe in cwe_order:
        if cwe in cwe_commit_workloads:
            print(f"* Author Commit Workloads ({cwe}):", file=output_file)
            print(cwe_commit_workloads[cwe], file=output_file)

## Author Churn Workload

### (Overall) Frequency Distribution
workloads = split_distribution(df_vccs, column="author_churn_workload")
with open(output_filepath, "a") as output_file:
    print("* Author Churn Workloads (Overall):", file=output_file)
    print(workloads, file=output_file)

### (Per CWEs) Frequency Distribution
cwe_churn_workloads = {}
for name, group_df in df_vccs_top_25_groups:
    cwe_churn_workloads[name] = split_distribution(group_df, column="author_churn_workload")
with open(output_filepath, "a") as output_file:
    for cwe in cwe_order:
        if cwe in cwe_churn_workloads:
            print(f"* Author Churn Workloads ({cwe}):", file=output_file)
            print(cwe_churn_workloads[cwe], file=output_file)

## Author Tenure

### (Overall) Frequency Distribution
tenures = split_distribution(df_vccs, column="author_tenure")
with open(output_filepath, "a") as output_file:
    print("==========\n", file=output_file)
    print("Author Tenure Results:", file=output_file)
    print("* Author Tenures (Overall):", file=output_file)
    print(tenures, file=output_file)

## (Per CWEs) Frequency Distribution
cwe_tenures = {}
for name, group_df in df_vccs_top_25_groups:
    cwe_tenures[name] = split_distribution(group_df, column="author_tenure")
with open(output_filepath, "a") as output_file:
    for cwe in cwe_order:
        if cwe in cwe_tenures:
            print(f"* Author Tenures ({cwe}):", file=output_file)
            print(cwe_tenures[cwe], file=output_file)

## CVE Analysis: Average Tenures vs. CVSS Severity
corr_tenure, pvalue_tenure = spearmanr(df_cves["author_tenure"], df_cves["severity"])
corr_commits, pvalue_commits = spearmanr(df_cves["author_commits_workload"], df_cves["severity"])
corr_churn, pvalue_churn = spearmanr(df_cves["author_churn_workload"], df_cves["severity"])
with open(output_filepath, "a") as output_file:
    print("==========\n", file=output_file)
    print(f"* Average Commits Workload vs. CVSS Severity - Spearman's Rho: {corr_commits} (p={pvalue_commits})", file=output_file)
    if pvalue_commits <= 0.05:
        print("  -> Significant Correlation", file=output_file)
    else:
        print("  -> No Significant Correlation", file=output_file)
    print(f"* Average Churn Workload vs. CVSS Severity - Spearman's Rho: {corr_churn} (p={pvalue_churn})", file=output_file)
    if pvalue_churn <= 0.05:
        print("  -> Significant Correlation", file=output_file)
    else:
        print("  -> No Significant Correlation", file=output_file)
    print(f"* Average Tenures vs. CVSS Severity - Spearman's Rho: {corr_tenure} (p={pvalue_tenure})", file=output_file)
    if pvalue_tenure <= 0.05:
        print("  -> Significant Correlation", file=output_file)
    else:
        print("  -> No Significant Correlation", file=output_file)
    