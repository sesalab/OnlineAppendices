import csv
import json
import sys
import os

import pandas as pd

sys.path.insert(0, "../common")
from commit_analysis import get_commit_goals
from utils import str_to_datetime


def compute_time_distances(row):
    date = str_to_datetime(row["date"])
    nearest_release_date = str_to_datetime(row["nearest_release_date"])
    creation_date = str_to_datetime(row["creation_date"])
    distance_from_release = nearest_release_date - date
    distance_from_creation = date - creation_date
    time_distances = {
        "distance_from_release": distance_from_release.days,
        "distance_from_creation": distance_from_creation.days
    }
    return pd.Series(time_distances)


def compute_days_distance(cve):
    last_fix_date = str_to_datetime(cve["last_fix_date"])
    turning_date = str_to_datetime(cve["turning_date"])
    return (last_fix_date - turning_date).days


def prepare_project_info(valid_repos, discarded_repos, projects_info_filepath, cwe_count_filepath, projects_summary_filepath, output_filepath):
    print("Preparing Projects Summary")
    projects = []
    cve_data = []
    for er in valid_repos:
        url = er["repo"]
        org_name = url.rsplit('/', 2)[1]
        project_name = url.rsplit('/', 2)[2]
        project_info = {
            "project": project_name,
            "org": org_name,
            "url": url,
            "commits": er["commits"],
            "authors": er["authors"],
            "stars": er["stars"],
            "forks": er["forks"],
            "language": er["language"],
            "new_feature": er["goals"]["new_feature"],
            "bug_fixing": er["goals"]["bug_fixing"],
            "enhancement": er["goals"]["enhancement"],
            "refactoring": er["goals"]["refactoring"],
            "cves": len(er["cves"])
        }
        projects.append(project_info)
        for cve in er["cves"]: 
            cve_data.append([
                cve["cwe"],
                cve["cve"],
            ])

    # Update CSV
    with open(projects_info_filepath, 'w') as out_file:
        writer = csv.DictWriter(out_file, fieldnames=projects[0].keys())
        writer.writeheader()
        writer.writerows(projects)

    invalid_repos = [d for d in discarded_repos if d["reason"] == "INVALID"]
    unavailable_repos = [d for d in discarded_repos if d["reason"] == "UNAVAILABLE"]
    num_valid_repos = len(projects)
    num_invalid_repos = len(invalid_repos)
    num_unavailable_repos = len(unavailable_repos)
    num_discarded_repos = num_invalid_repos + num_unavailable_repos
    num_total_repos = num_valid_repos + num_discarded_repos

    cols = ["cwe", "cve"]
    df_cves = pd.DataFrame(cve_data, columns=cols)
    df_projects = pd.DataFrame(projects, columns=projects[0].keys())
    df_projects.describe().to_csv(projects_summary_filepath)
    cwe_counts = df_cves.groupby("cwe").count()
    cwe_counts.rename(columns={"cve": "count"}, inplace=True)
    cwe_counts.sort_values(by="count", ascending=False, inplace=True)
    num_cves = df_cves["cve"].nunique()
    cwe_counts["perc"] = cwe_counts["count"] / num_cves
    num_cwes = len(cwe_counts)

    with open(output_filepath, "a") as output_file:
        print("Projects Summary:", file=output_file)
        print(f"* Nr. Valid Repositories: {num_valid_repos}/{num_total_repos}", file=output_file)
        print(f"* Nr. Discarded Repositories: {num_discarded_repos}/{num_total_repos}", file=output_file)
        print(f"  - Unavailable (Could not be cloned): {num_unavailable_repos}/{num_discarded_repos}", file=output_file)
        print(f"  - Invalid (No valid Fixes or VCCs): {num_invalid_repos}/{num_discarded_repos}", file=output_file)
        print(f"* Nr. CVEs: {num_cves}", file=output_file)
        print(f"* Nr. CWEs: {num_cwes}", file=output_file)
        print("", file=output_file)

    cwe_counts.to_csv(cwe_count_filepath)
    print("Projects Summary Done!")
    print()


def prepare_rq1(repos, out_filepath, output_filepath):
    print("Preparing Input Dataset for RQ1")
    data_rq1 = []
    for repo in repos:
        for cve in repo["cves"]:
            for fix in cve["fixes"]:
                for vcc in fix["vccs"]:
                    for file in vcc["files"]:
                        row = [
                            repo["repo"],
                            vcc["hash"],
                            vcc["date"],
                            cve["cve"],
                            cve["cwe"],
                            file["name"],
                            vcc["added_lines"],
                            vcc["removed_lines"],
                            file["previous_changes"]
                        ]
                        data_rq1.append(row)
    cols = ["repo", "hash", "date", "cve", "cwe", "file", "added_lines", "removed_lines", "previous_changes"]
    df_rq1 = pd.DataFrame(data_rq1, columns=cols)

    ## Drop duplicates
    old_len = len(df_rq1)
    df_rq1.drop_duplicates(ignore_index=True, inplace=True)
    new_len = len(df_rq1)

    with open(output_filepath, "a") as output_file:
        print("RQ1 (Each entry represents a pair {VCC, changed file}):", file=output_file)
        print(f"* Removed {old_len - new_len}/{old_len} Duplicated Pairs (due to same fixes for different CVEs)", file=output_file)
        print(f"* Final Nr. Pairs: {new_len}", file=output_file)
        print("", file=output_file)

    df_rq1.to_csv(out_filepath, index=False)
    print("RQ1 Done!")
    print()


def prepare_rq2(repos, out_valid_filepath, out_discarded_filepath, output_filepath):
    print("Preparing Input Dataset for RQ2")
    data_rq2 = []
    for repo in repos:
        for cve in repo["cves"]:
            for fix in cve["fixes"]:
                for vcc in fix["vccs"]:
                    row = [
                        repo["repo"],
                        vcc["hash"],
                        repo["creation_date"],
                        vcc["message"],
                        vcc["author"],
                        vcc["author_commits_workload"],
                        vcc["author_churn_workload"],
                        vcc["author_tenure"],
                        vcc["date"],
                        vcc["nearest_release_date"],
                        cve["cve"],
                        cve["cwe"],
                        cve["cvss"]
                    ]
                    data_rq2.append(row)
    cols = ["repo", "hash", "creation_date", "message", "author", "author_commits_workload", "author_churn_workload", "author_tenure", "date", "nearest_release_date", "cve", "cwe", "cvss"]
    df_rq2 = pd.DataFrame(data_rq2, columns=cols)

    ## Drop duplicates
    old_len = len(df_rq2)
    df_rq2.drop_duplicates(subset=["repo", "hash"], ignore_index=True, inplace=True)
    new_len = len(df_rq2)

    ## Discard entries that have "" as nearest_release_date (sadly, we are forced to do so)
    initial_num_vccs = df_rq2.groupby(["repo", "hash"]).size().count()
    df_discarded_rq2 = df_rq2[df_rq2["nearest_release_date"] == ""]
    df_rq2 = df_rq2[df_rq2["nearest_release_date"] != ""]

    ## Commit goals categorisation => for each VCC, check the message and add 4 new columns representing the commit goal 
    commit_goals = df_rq2.apply(lambda row: pd.Series(get_commit_goals(row["message"])), axis=1)
    df_rq2 = pd.concat([df_rq2, commit_goals], axis=1)

    ## Project status => for each VCC, compute the difference between date and [nearest_release_date/creation_date]
    project_status = df_rq2.apply(lambda row: compute_time_distances(row), axis=1)
    df_rq2 = pd.concat([df_rq2, project_status], axis=1)

    ## Discard entries that have negative distance_from_release (due to rebase or amends)
    intermediate_num_vccs = df_rq2.groupby(["repo", "hash"]).size().count()
    df_discarded_rq2.append(df_rq2[df_rq2["distance_from_release"] < 0])
    df_rq2 = df_rq2[df_rq2["distance_from_release"] >= 0]
    num_vccs = df_rq2.groupby(["repo", "hash"]).size().count()

    with open(output_filepath, "a") as output_file:
        print("RQ2 (Each entry represents a VCC):", file=output_file)
        print(f"* Removed {old_len - new_len}/{old_len} Duplicated VCCs (due to fixes sharing the same VCCs)", file=output_file)
        print(f"* Removed {initial_num_vccs - num_vccs}/{new_len} Additional VCCs", file=output_file)
        print(f"  - No Nearest Release Date: {initial_num_vccs - intermediate_num_vccs}/{initial_num_vccs - num_vccs}", file=output_file)
        print(f"  - Amended or Rebased: {intermediate_num_vccs - num_vccs}/{initial_num_vccs - num_vccs}", file=output_file)
        print(f"* Final Nr. VCCs: {new_len}", file=output_file)
        print("", file=output_file)

    df_rq2.to_csv(out_valid_filepath, index=False)
    df_discarded_rq2.to_csv(out_discarded_filepath, index=False)
    print("RQ2 Done!")
    print()


def prepare_rq3(repos, out_valid_filepath, out_discarded_filepath, output_filepath):
    print("Preparing Input Dataset for RQ3")
    fixes = []
    vccs = []
    for repo in repos:
        for cve in repo["cves"]:
            for fix in cve["fixes"]:
                fixes.append([
                    repo["repo"],
                    repo["language"],
                    repo["stars"],
                    cve["cve"],
                    fix["hash"],
                    fix["date"],
                    len(fix["vccs"])
                ])
                for vcc in fix["vccs"]:
                    vccs.append([
                        repo["repo"],
                        cve["cve"],
                        cve["cwe"],
                        vcc["hash"],
                        vcc["date"],
                        fix["date"],
                        vcc["changes_before_fix"],
                        fix["changes_before_last_fix"]
                    ])
    fixes_cols = ["repo", "language", "stars", "cve", "hash", "date", "num_vccs"]
    vcc_cols = ["repo", "cve", "cwe", "hash", "vcc_date", "fix_date", "changes_before_fix", "changes_between_fix_and_last_fix"]
    df_rq3_fixes = pd.DataFrame(fixes, columns=fixes_cols)
    df_rq3_vccs = pd.DataFrame(vccs, columns=vcc_cols)

    ## Do not consider fixes without VCCs
    df_rq3_fixes = df_rq3_fixes[df_rq3_fixes["num_vccs"] != 0]

    ## Group by CVE and get all CVE turning points and last fixes
    vccs_cve_groups = df_rq3_vccs.groupby("cve", sort=False)
    fixes_cve_groups = df_rq3_fixes.groupby("cve", sort=False)
    df_turnings = df_rq3_vccs[vccs_cve_groups["vcc_date"].transform(max) == df_rq3_vccs["vcc_date"]]
    df_last_fixes = df_rq3_fixes[fixes_cve_groups["date"].transform(max) == df_rq3_fixes["date"]]

    ## In cases where CVE has multiple fixes that shares the same turning point, we consider the first one
    df_turnings = df_turnings.drop_duplicates(subset=["cve"], ignore_index=True)

    ## In cases where CVE has multiple fixes with same date, we consider the first one
    df_last_fixes = df_last_fixes.drop_duplicates(subset=["cve"], ignore_index=True)

    ## Join the two dataframes on CVE column
    df_rq3 = pd.merge(df_turnings, df_last_fixes, on="cve")
    df_rq3.rename({
        "hash_x": "turning_hash",
        "hash_y": "last_fix_hash",
        "repo_x": "repo",
        "vcc_date": "turning_date",
        "date": "last_fix_date"
    }, axis=1, inplace=True)

    ## Compute the time/commits differences
    df_rq3["days_before_last_fix"] = df_rq3.apply(lambda row: compute_days_distance(row), axis=1)
    df_rq3["changes_before_last_fix"] = df_rq3["changes_before_fix"] + df_rq3["changes_between_fix_and_last_fix"]
    df_rq3 = df_rq3[["cve", "cwe", "repo", "language", "stars", "turning_hash", "last_fix_hash", "days_before_last_fix", "changes_before_last_fix"]]

    ## Discard negative time cases
    old_len = len(df_rq3)
    df_discarded_rq3 = df_rq3[df_rq3["days_before_last_fix"] < 0]
    df_rq3 = (df_rq3[df_rq3["days_before_last_fix"] >= 0]).reset_index(drop=True)

    with open(output_filepath, "a") as output_file:
        print("RQ3 (Each entry represents a CVE):", file=output_file)
        print(f"* Removed {len(df_discarded_rq3)}/{old_len} CVEs (due to rebases or amends)", file=output_file)
        print(f"* Final Nr. CVEs: {len(df_rq3)}", file=output_file)
        print("", file=output_file)

    df_rq3.to_csv(out_valid_filepath, index=False)
    df_discarded_rq3.to_csv(out_discarded_filepath, index=False)
    print("RQ3 Done!")
    print()


def prepare_rq4(repos, out_filepath, output_filepath):
    print("Preparing Input Dataset for RQ4")
    fix_cves = []
    for repo in repos:
        for cve in repo["cves"]:
            cve_fixes_number = len([fix for fix in cve["fixes"]])
            cve_fixes_files = set()
            for fix in cve["fixes"]:
                cve_fixes_files.update(fix["files"])
            cve_fixes_added_lines = sum((fix["added_lines"] for fix in cve["fixes"]))
            cve_fixes_removed_lines = sum((fix["removed_lines"] for fix in cve["fixes"]))
            for fix in cve["fixes"]:
                fix_files = len(fix["files"])
                fix_added_lines = fix["added_lines"]
                fix_removed_lines = fix["removed_lines"]
                fix_vccs = len(fix["vccs"])
                fix_changes_before_last_fix = fix["changes_before_last_fix"]
                fix_cves.append([
                    fix["hash"],
                    cve["cve"],
                    cve["cwe"],
                    repo["language"],
                    fix_files,
                    fix_added_lines,
                    fix_removed_lines,
                    fix_vccs,
                    fix_changes_before_last_fix,
                    cve_fixes_number,
                    len(cve_fixes_files),
                    cve_fixes_added_lines,
                    cve_fixes_removed_lines
                ])
    fix_cves_cols = ["fixHash", "cve", "cwe", "language", "fix_files", "fix_added_lines", "fix_removed_lines", "fix_vccs", "fix_changes_before_last_fix", "cve_fixes_number", "cve_fixes_files", "cve_fixes_added_lines", "cve_fixes_removed_lines"]
    df_fix_cves = pd.DataFrame(fix_cves, columns=fix_cves_cols)
    df_removal = pd.read_csv(removal_methods_filepath)
    df_removal.drop(["fixUrl", "cveUrl"], axis=1, inplace=True)
    
    ## Join the two dataframes on CVE column
    df_rq4 = pd.merge(df_removal, df_fix_cves, on=["fixHash", "cve"], how="left")
    df_rq4.drop_duplicates(ignore_index=True, inplace=True)

    with open(output_filepath, "a") as output_file:
        print("RQ4 (Each entry represents a sampled pair {fix, CVE}):", file=output_file)
        print(f"* Final Nr. sample pairs: {len(df_rq4)}", file=output_file)

    df_rq4.to_csv(out_filepath, index=False)
    print("RQ4 Done!")
    print()


if len(sys.argv) < 2:
    print("Need to specify the directory containing the input data. Exiting.")
    sys.exit(1)
input_dir = sys.argv[1] # e.g., ../../../data/
if len(sys.argv) < 3:
    print("Need to specify the directory containing the output data. Exiting.")
    sys.exit(1)
output_dir = sys.argv[2] # e.g., ../../../results/

enriched_filepath = os.path.join(input_dir, "repos_enriched.json")
discarded_filepath = os.path.join(input_dir, "repos_discarded.json")
projects_info_filepath = os.path.join(input_dir, "projects_info.csv")

projects_summary_filepath = os.path.join(output_dir, "data_preparation/projects_summary.csv")
cwe_count_filepath = os.path.join(output_dir, "data_preparation/cwe_count.csv")
output_filepath = os.path.join(output_dir, "data_preparation/output.txt")

rq1_input = os.path.join(input_dir, "rq1_input.csv")
rq2_input = os.path.join(input_dir, "rq2_input.csv")
rq3_input = os.path.join(input_dir, "rq3_input.csv")
rq4_input = os.path.join(input_dir, "rq4_input.csv")
rq2_discarded = os.path.join(input_dir, "rq2_discarded.csv")
rq3_discarded = os.path.join(input_dir, "rq3_discarded.csv")
removal_methods_filepath = os.path.join(input_dir, "removal_methods.csv")

enriched_repos = []
with open(enriched_filepath, "r") as in_file:
    enriched_repos = json.load(in_file)

discarded_repos = []
with open(discarded_filepath, "r") as in_file:
    discarded_repos = json.load(in_file)

open(output_filepath, "w").close()

prepare_project_info(enriched_repos, discarded_repos, projects_info_filepath, cwe_count_filepath, projects_summary_filepath, output_filepath)
prepare_rq1(enriched_repos, rq1_input, output_filepath)
prepare_rq2(enriched_repos, rq2_input, rq2_discarded, output_filepath)
prepare_rq3(enriched_repos, rq3_input, rq3_discarded, output_filepath)
prepare_rq4(enriched_repos, rq4_input, output_filepath)
