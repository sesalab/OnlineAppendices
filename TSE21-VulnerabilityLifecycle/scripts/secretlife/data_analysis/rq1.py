import os
import sys

import numpy as np
import pandas as pd
from scipy import stats

sys.path.insert(0, "../common")
from utils import show_boxplots

if len(sys.argv) < 2:
    print("Need to specify the directory containing the input data. Exiting.")
    sys.exit(1)
input_dir = sys.argv[1] # e.g., ../../../data/
if len(sys.argv) < 3:
    print("Need to specify the directory containing the output data. Exiting.")
    sys.exit(1)
base_output_dir = os.path.join(sys.argv[2], "rq1") # e.g., ../../../results/

input_filepath = os.path.join(input_dir, "rq1_input.csv")
top_25_filepath = os.path.join(input_dir, "top25cwe.csv")

df_vccs = pd.read_csv(input_filepath, delimiter=",")
top_25 = pd.read_csv(top_25_filepath)[["cwe", "position"]]

def run_analysis(output_dir, df_vccs, top_25):
    output_filepath = os.path.join(output_dir, "output.txt")

    vccs_per_cve_overall_summary_filepath = os.path.join(output_dir, "overall/vccs_per_cve.csv")
    vccs_ratio_per_project_overall_summary_filepath = os.path.join(output_dir, "overall/vccs_ratio_per_project.csv")
    insertion_window_per_cve_overall_summary_filepath = os.path.join(output_dir, "overall/insertion_window_per_cve.csv")
    vccs_per_cve_cwe_summary_filepath = os.path.join(output_dir, "cwes/vccs_per_cve.csv")
    vccs_per_cve_cwe_boxplots_filepath = os.path.join(output_dir, "cwes/boxplots/vccs_per_cve_cwe_{}.pdf")

    files_per_vcc_overall_summary_filepath = os.path.join(output_dir, "overall/files_per_vcc.csv")
    added_lines_per_vcc_overall_summary_filepath = os.path.join(output_dir, "overall/added_lines_per_vcc.csv")
    removed_lines_per_vcc_overall_summary_filepath = os.path.join(output_dir, "overall/removed_lines_per_vcc.csv")
    files_per_vcc_cwe_summary_filepath = os.path.join(output_dir, "cwes/files_per_vcc.csv")
    files_per_vcc_cwe_boxplots_filepath = os.path.join(output_dir, "cwes/boxplots/files_per_vcc_cwe_{}.pdf")
    files_previous_changes_first_vcc_summary_filepath = os.path.join(output_dir, "overall/files_previous_changes_first_vcc.csv")
    files_previous_changes_last_vcc_summary_filepath = os.path.join(output_dir, "overall/files_previous_changes_last_vcc.csv")

    df_vccs_top_25 = df_vccs[df_vccs["cwe"].isin(top_25["cwe"])]
    cve_groups = df_vccs.groupby("cve", sort=False)
    project_groups = df_vccs.groupby("repo", sort=False)
    vcc_groups = df_vccs.groupby(["repo", "hash"], sort=False)

    # General Profiling
    num_vccs = df_vccs.groupby(["repo", "hash"]).size().count()
    num_repos = len(project_groups)
    num_cwes = df_vccs["cwe"].nunique()
    num_files = df_vccs.groupby(["repo", "file"]).size().count()

    # Part 1: VCCs per CVE

    ## (Overall) Summary
    num_cves = len(cve_groups)
    vccs_per_cve = cve_groups["hash"].nunique()
    vccs_per_cve.describe().to_csv(vccs_per_cve_overall_summary_filepath)

    ## (Overall) VCCs Ratio per Project
    project_num_vccs = project_groups.size().rename("vccs").to_frame().reset_index()
    project_num_commits = project_groups.first()["repo_commits"].to_frame().reset_index()
    projects = project_num_vccs.merge(project_num_commits, on="repo")
    projects["vccs_ratio"] = projects["vccs"] / projects["repo_commits"]
    projects.describe().to_csv(vccs_ratio_per_project_overall_summary_filepath)

    ## (Overall) CVEs with > 1 VCCs
    cve_groups_many_vccs = cve_groups.filter(lambda g: g["hash"].nunique() > 1).groupby("cve", sort=False)
    cve_with_many_vccs_count = len(cve_groups_many_vccs)
    cve_with_many_vccs_perc = cve_with_many_vccs_count / num_cves * 100

    ## (Overall) Insertion window analysis
    first_vccs = cve_groups_many_vccs["date"].min().to_frame().reset_index().rename(columns={"date": "first_date"})
    last_vccs = cve_groups_many_vccs["date"].max().to_frame().reset_index().rename(columns={"date": "last_date"})
    cves_windows = first_vccs.merge(last_vccs, on="cve")
    cves_windows[["first_date", "last_date"]] = cves_windows[["first_date", "last_date"]].apply(pd.to_datetime)
    cves_windows["days_distance"] = (cves_windows["last_date"] - cves_windows["first_date"]).dt.days
    window_duration_mean = cves_windows["days_distance"].mean()
    cves_windows["days_distance"].describe().to_csv(insertion_window_per_cve_overall_summary_filepath)

    ## (Overall) CVE w/ largest number of VCCs
    vccs_per_cve_idxmax = vccs_per_cve.idxmax()
    vccs_per_cve_max = vccs_per_cve.max()
    cve_max_vccs = cve_groups.get_group(vccs_per_cve_idxmax)
    cve_max_vccs_files_count = cve_max_vccs["file"].nunique()
    cve_max_vccs_added_lines = cve_max_vccs["added_lines"].sum()
    cve_max_vccs_removed_lines = cve_max_vccs["removed_lines"].sum()
    cve_max_vccs_window_duration = cves_windows.loc[cves_windows["cve"] == vccs_per_cve_idxmax]["days_distance"].values[0]

    ## (Overall) CVEs w/ shortest and largest insertion window
    cves_windows_max = cves_windows.loc[cves_windows["days_distance"].idxmax()]
    cves_windows_max_cve = cves_windows_max["cve"]
    cves_windows_max_duration = cves_windows_max["days_distance"]
    cves_windows_min = cves_windows.loc[cves_windows["days_distance"].idxmin()]
    cves_windows_min_cve = cves_windows_min["cve"]
    cves_windows_min_duration = cves_windows_min["days_distance"]

    ## (Per CWEs) Summary and Boxplots

    ### Makes the "double" groupby and then orders according to the Top 25 order
    vccs_per_cve_cwe = df_vccs_top_25 \
        .groupby("cwe", sort=False)[["hash", "cve"]] \
        .apply(lambda x: x.groupby("cve", sort=False).nunique()) \
        .reset_index() \
        .merge(top_25, how="left", on="cwe") \
        .sort_values("position", ignore_index=True) \
        .drop(columns=["cve", "position"])
    vccs_per_cve_cwe.groupby("cwe", sort=False)["hash"].describe().to_csv(vccs_per_cve_cwe_summary_filepath)
    show_boxplots(vccs_per_cve_cwe, "cwe", "hash", "VCCs per CVE", vccs_per_cve_cwe_boxplots_filepath, log=True)

    # Part 2: Touched Files per VCC

    ## (Overall) Summary
    files_per_vcc = vcc_groups["file"].nunique()
    files_per_vcc.describe().to_csv(files_per_vcc_overall_summary_filepath)

    ## (Overall) Code Churn
    added_lines_per_vcc = vcc_groups["added_lines"].sum()
    added_lines_per_vcc.describe().to_csv(added_lines_per_vcc_overall_summary_filepath)
    removed_lines_per_vcc = vcc_groups["removed_lines"].sum()
    removed_lines_per_vcc.describe().to_csv(removed_lines_per_vcc_overall_summary_filepath)

    ## (Overall) VCCs touching > 10 files
    vccs_more_ten_files_count = files_per_vcc[files_per_vcc > 10].count()
    vccs_more_ten_files_perc = vccs_more_ten_files_count / num_vccs * 100

    ## (Overall) VCC w/ largest number of Files
    files_per_vcc_idxmax = files_per_vcc.idxmax()
    files_per_vcc_idxmax_repo = files_per_vcc_idxmax[0]
    files_per_vcc_idxmax_hash = files_per_vcc_idxmax[1]
    files_per_vcc_max = files_per_vcc.max()
    vcc_max_files = vcc_groups.get_group(files_per_vcc_idxmax)
    vcc_max_files_count = vcc_max_files["file"].nunique()
    vcc_max_cve_counts = vcc_max_files["cve"].value_counts()
    vcc_max_files_added_lines = vcc_max_files["added_lines"].iloc[0]
    vcc_max_files_removed_lines = vcc_max_files["removed_lines"].iloc[0]

    ## (Per CWEs) Summary and Boxplots
    files_per_vcc_cwe = df_vccs_top_25 \
        .groupby("cwe", sort=False)[["repo", "hash", "file"]] \
        .apply(lambda x: x.groupby(["repo", "hash"], sort=False).nunique()) \
        .reset_index() \
        .merge(top_25, how="left", on="cwe") \
        .sort_values("position", ignore_index=True) \
        .drop(columns=["repo", "hash", "position"])
    files_per_vcc_cwe.groupby("cwe", sort=False)["file"].describe().to_csv(files_per_vcc_cwe_summary_filepath)
    show_boxplots(files_per_vcc_cwe, "cwe", "file", "Touched Files per VCC", files_per_vcc_cwe_boxplots_filepath, log=True)

    # Part 3: Number of Previous Changes for files in First/Last VCC

    ## (Overall) Summary

    df_vccs_first = df_vccs[cve_groups["date"].transform(min) == df_vccs["date"]]
    previous_changes_vccs_first = df_vccs_first["previous_changes"]
    df_vccs_last = df_vccs[cve_groups["date"].transform(max) == df_vccs["date"]]
    previous_changes_vccs_last = df_vccs_last["previous_changes"]
    previous_changes_vccs_first.describe().to_csv(files_previous_changes_first_vcc_summary_filepath)
    previous_changes_vccs_last.describe().to_csv(files_previous_changes_last_vcc_summary_filepath)

    # Part 4: Files that were created in a VCC (i.e., previous_changes == 0) + Files that were created in a VCC that is the sole the CVE
    files_created_vcc = df_vccs.loc[df_vccs["previous_changes"] == 0, ["repo", "file"]].drop_duplicates(ignore_index=True)
    files_cves_one_vcc = cve_groups.filter(lambda x: len(x) == 1)[["repo", "file"]].drop_duplicates(ignore_index=True)
    files_cves_one_vcc_created = pd.merge(files_created_vcc, files_cves_one_vcc, on=["repo", "file"])
    files_created_vcc_count = len(files_created_vcc)
    files_cves_one_vcc_created_count = len(files_cves_one_vcc_created)

    # Part 5: Files that were changed > 500 times before VCC
    files_first_vcc = df_vccs[df_vccs.groupby(["repo", "file"], sort=False)["previous_changes"].transform(min) == df_vccs["previous_changes"]]
    files_over_500 = files_first_vcc.loc[files_first_vcc["previous_changes"] > 500, ["repo", "file", "previous_changes"]].sort_values(by="previous_changes", ascending=False, ignore_index=True)
    files_over_500_count = len(files_over_500)

    with open(output_filepath, "w") as output_file:
        print(f"RQ1 Context:", file=output_file)
        print(f"* CVEs: {num_cves}", file=output_file)
        print(f"* VCCs: {num_vccs}", file=output_file)
        print(f"* Repos: {num_repos}", file=output_file)
        print(f"* CWEs: {num_cwes}", file=output_file)
        print(f"* Files (distinct): {num_files}", file=output_file)
        print(file=output_file)
        print(f"RQ1 Main Results:", file=output_file)
        print(f"* CVEs with > 1 VCC: {cve_with_many_vccs_count}/{num_cves} ({round(cve_with_many_vccs_perc, 2)}%)", file=output_file)
        print(f"* Mean Nr. Days between first and last VCCs: {round(window_duration_mean, 3)} ({round(window_duration_mean/365, 3)} years)", file=output_file)
        print(f"* {vccs_per_cve_idxmax} has the largest number of VCCs: {vccs_per_cve_max}. Considering all VCCs:", file=output_file)
        print(f"  - Touched a total of {cve_max_vccs_files_count} valid files", file=output_file)
        print(f"  - Added a total of {cve_max_vccs_added_lines} lines", file=output_file)
        print(f"  - Removed a total of {cve_max_vccs_removed_lines} lines", file=output_file)
        print(f"  - Taking {cve_max_vccs_window_duration} days ({round(cve_max_vccs_window_duration/365, 3)} years) between first and last VCCs", file=output_file)
        print(f"* {cves_windows_max_cve} has the largest insertion window: {cves_windows_max_duration} days ({round(cves_windows_max_duration/365, 3)} years)", file=output_file)
        print(f"* {cves_windows_min_cve} has the shortest insertion window: {cves_windows_min_duration} days ({round(cves_windows_min_duration/365, 3)} years)", file=output_file)
        print(file=output_file)
        print(f"* VCCs that touched > 10 files: {vccs_more_ten_files_count}/{num_vccs} ({round(vccs_more_ten_files_perc, 2)}%)", file=output_file)
        print(f"* VCC {files_per_vcc_idxmax_hash} of {files_per_vcc_idxmax_repo} touched the largest number of valid files: {files_per_vcc_max}. Specifically:", file=output_file)
        print(f"  - Contributed to: {vcc_max_cve_counts.index.values}", file=output_file) 
        print(f"  - Touched {vcc_max_files_count} valid files", file=output_file)
        print(f"  - Added {vcc_max_files_added_lines} lines", file=output_file)
        print(f"  - Removed {vcc_max_files_removed_lines} lines", file=output_file)
        print(f"* Files created within a VCC: {files_created_vcc_count}/{num_files} ({round(files_created_vcc_count / num_files * 100, 2)}%)", file=output_file)
        print(f"  - Files created in \"single-VCC\" vulnerabilities: {files_cves_one_vcc_created_count}/{files_created_vcc_count} ({round(files_cves_one_vcc_created_count / files_created_vcc_count * 100, 2)}%)", file=output_file)
        print(f"* Files that started becoming vulnerable after >= 500 commits: {files_over_500_count}/{num_files} ({round(files_over_500_count / num_files * 100, 2)}%)", file=output_file)

run_analysis(os.path.join(base_output_dir, "main"), df_vccs, top_25)

# Drop outliers: VCCS whose CVE has lots of VCCS
cve_groups = df_vccs.groupby("cve", sort=False)
vccs_per_cve = cve_groups["hash"].nunique()
large_cves = vccs_per_cve.loc[np.abs(stats.zscore(vccs_per_cve)) > 3].index.tolist()
df_vccs_no_large_cves = df_vccs.loc[~df_vccs["cve"].isin(large_cves)]
run_analysis(os.path.join(base_output_dir, "no_large_cves"), df_vccs_no_large_cves, top_25)

# Drop outliers: VCCS having touched lots of files
vcc_groups = df_vccs.groupby(["repo", "hash"], sort=False)
files_per_vcc = vcc_groups["file"].nunique()
large_vccs = files_per_vcc.loc[np.abs(stats.zscore(files_per_vcc)) > 3].index.tolist()
large_vccs = [lv[0] + "/commit/" + lv[1] for lv in large_vccs]
tmp_df_vccs = df_vccs.copy()
tmp_df_vccs["vcc"] = df_vccs["repo"] + "/commit/" + df_vccs["hash"]
df_vccs_no_large_vccs = tmp_df_vccs.loc[~tmp_df_vccs["vcc"].isin(large_vccs)]
run_analysis(os.path.join(base_output_dir, "no_large_vccs"), df_vccs_no_large_vccs, top_25)
